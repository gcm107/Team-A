{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd458681",
   "metadata": {},
   "source": [
    "### Current Idea (open to other ideas of course!):\n",
    "\n",
    "\n",
    "Traditional factor models can struggle capturing short-horizon \"micro-regimes\" in stock price behavior. The Goal of this project is to use a small autoencoder to learn regimes from 20-day *(I just picked a number we can change this obviously)* sliding windows of price/volume features, and then studiy:\n",
    "\n",
    "- What the regimes look like\n",
    "- How they transition from one to another\n",
    "- Whether transitions have predictive power for near-term returns or volatitlity\n",
    "- Potential comparissons to simple baselines (momentum, volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576834f",
   "metadata": {},
   "source": [
    "### Gathering Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15fa82",
   "metadata": {},
   "source": [
    "####  Optional Download (You shouldn't need to do this) Please use the link in the readme to get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cbb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download data. \n",
    "# Not needed if you have the .parquet file in the data/historical folder\n",
    "\n",
    "# !python qualifier\\utils\\download_stock_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acdfcd",
   "metadata": {},
   "source": [
    "#### Load Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data path\n",
    "PARQUET_PATH = Path(\"data/historical/all_stocks_historical.parquet\")\n",
    "\n",
    "print(f\"Loading data from: {PARQUET_PATH}\")\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize and prepare data\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by ticker and date\n",
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Data Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Days covered: {(df['date'].max() - df['date'].min()).days:,} days\")\n",
    "print(f\"Date column type: {df['date'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677f9e0",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3020e",
   "metadata": {},
   "source": [
    "#### Handling Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16607872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data with nans\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Original Data:\")\n",
    "print(\"=\" * 60)\n",
    "display(df.head())\n",
    "\n",
    "# no nan \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"valid (non-null) data:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# remove rows with nans\n",
    "cleaned_df = df.dropna(subset=['close', 'volume'])\n",
    "display(cleaned_df[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38041b1",
   "metadata": {},
   "source": [
    "#### Trimming dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed685e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming the df_valid to start at the year 2000\n",
    "cleaned_trimmed_df = cleaned_df[cleaned_df['date'] >= '2000-01-01'].copy()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Filtered data (Year 2000 onwards):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(cleaned_trimmed_df):,}\")\n",
    "print(f\"Date range: {cleaned_trimmed_df['date'].min().date()} to {cleaned_trimmed_df['date'].max().date()}\")\n",
    "display(cleaned_trimmed_df[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d90b8c",
   "metadata": {},
   "source": [
    "#### Removing stocks that do not have at least 1000 days of data and Filtering for highest volume stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two-stage universe selection/filtering to shrink for MVP\n",
    "# 1. remove tickers with insufficient history\n",
    "# 2. select top N by average volume from quality-filtered set\n",
    "\n",
    "\n",
    "min_days_required = 1000  # min number of days per ticker\n",
    "top_n = 50  # target universe size\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Stage 1: Quality Filter (min {min_days_required} trading days)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# count days per ticker in the full trimmed dataset\n",
    "rows_per_ticker_all = cleaned_trimmed_df.groupby('ticker').size()\n",
    "valid_tickers = rows_per_ticker_all[rows_per_ticker_all >= min_days_required].index.tolist()\n",
    "\n",
    "print(f\"Tickers before quality filter: {len(rows_per_ticker_all):,}\")\n",
    "print(f\"Tickers after quality filter:  {len(valid_tickers):,}\")\n",
    "print(f\"Tickers removed: {len(rows_per_ticker_all) - len(valid_tickers):,}\")\n",
    "\n",
    "# filter to quality tickers only\n",
    "quality_filtered_df = cleaned_trimmed_df[cleaned_trimmed_df['ticker'].isin(valid_tickers)].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Stage 2: Liquidity Ranking (top {top_n} by avg volume)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# average volume on quality-filtered stocks\n",
    "avg_volume_by_ticker = quality_filtered_df.groupby('ticker')['volume'].mean().sort_values(ascending=False)\n",
    "\n",
    "# select top N from the filtered universe\n",
    "universe = avg_volume_by_ticker.head(top_n).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data dataframe from the quality filters.\n",
    "data = quality_filtered_df[quality_filtered_df['ticker'].isin(universe)].copy()\n",
    "data = data.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected universe size: {len(universe)} tickers (target: {top_n})\")\n",
    "print(f\"{len(universe)} tickers have >= {min_days_required} days of history\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "for i, (ticker, vol) in enumerate(avg_volume_by_ticker.head(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {ticker:6s} - {vol:>15,.0f} shares/day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a679c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Final Universe: {len(universe)} stocks\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(data):,}\")\n",
    "print(f\"Unique tickers: {data['ticker'].nunique()}\")\n",
    "print(f\"Date range: {data['date'].min().date()} to {data['date'].max().date()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Data Completeness (per ticker):\")\n",
    "print(\"-\" * 60)\n",
    "rows_per_ticker = data.groupby('ticker').size()\n",
    "print(f\"  Min:    {rows_per_ticker.min():>5,} days  (>= {min_days_required} guaranteed)\")\n",
    "print(f\"  Median: {rows_per_ticker.median():>5,.0f} days\")\n",
    "print(f\"  Mean:   {rows_per_ticker.mean():>5,.0f} days\")\n",
    "print(f\"  Max:    {rows_per_ticker.max():>5,} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Selected Tickers (sorted by avg volume):\")\n",
    "print(f\"  {', '.join(universe)}\")\n",
    "\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fb043",
   "metadata": {},
   "source": [
    "### Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fba8b",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = data.copy()\n",
    "feat_df = feat_df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "# 1. log price & 1-day log return \n",
    "feat_df['log_price'] = np.log(feat_df['close'])\n",
    "feat_df['log_ret_1d'] = feat_df.groupby('ticker')['log_price'].diff()\n",
    "\n",
    "# 2. rolling volatilities of returns \n",
    "feat_df['vol_5d'] = (\n",
    "    feat_df.groupby('ticker')['log_ret_1d']\n",
    "           .rolling(window=5, min_periods=5)\n",
    "           .std()\n",
    "           .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "feat_df['vol_20d'] = (\n",
    "    feat_df.groupby('ticker')['log_ret_1d']\n",
    "           .rolling(window=20, min_periods=20)\n",
    "           .std()\n",
    "           .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 3. vol z-score with 1-period lag to exclude current row\n",
    "g = feat_df.groupby('ticker')['volume']\n",
    "exp_mean = g.expanding().mean().shift(1).reset_index(level=0, drop=True)\n",
    "exp_std  = g.expanding().std(ddof=0).shift(1).reset_index(level=0, drop=True)\n",
    "\n",
    "feat_df['volume_mean_exp'] = exp_mean\n",
    "feat_df['volume_std_exp']  = exp_std\n",
    "\n",
    "# avoid dividing by 0\n",
    "feat_df['volume_z'] = (\n",
    "    (feat_df['volume'] - feat_df['volume_mean_exp']) \n",
    "    / (feat_df['volume_std_exp'] + 1e-8)\n",
    ")\n",
    "\n",
    "\n",
    "# 4. intraday range\n",
    "feat_df['range_frac'] = (feat_df['high'] - feat_df['low']) / feat_df['close']\n",
    "\n",
    "# Keep only needed columns\n",
    "feature_cols = ['log_ret_1d', 'vol_5d', 'vol_20d', 'volume_z', 'range_frac']\n",
    "\n",
    "feat_df = feat_df[['ticker', 'date'] + feature_cols]\n",
    "\n",
    "# drop nan rows that were created because of frolling features etc\n",
    "feat_df = feat_df.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"Feature DF shape:\", feat_df.shape)\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6d221",
   "metadata": {},
   "source": [
    "#### Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "corr = feat_df[feature_cols].corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr,\n",
    "    text_auto=\".2f\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    origin=\"lower\",\n",
    "    title=\"Correlation between features\"\n",
    ")\n",
    "fig.update_layout(height=500, width=600)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd324e1e",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa87db",
   "metadata": {},
   "source": [
    "####  Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dd610",
   "metadata": {},
   "source": [
    "##### Creating windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03239476",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 20 # how big (how many days) the window is\n",
    "\n",
    "\n",
    "# function to make le windows of 20 days for each ticker\n",
    "def make_window_features(feat_df, feature_cols, window_len=20):\n",
    "    \n",
    "    \n",
    "    frames = []\n",
    "    # looping through the tickers and creating windows\n",
    "    for ticker, g in feat_df.groupby('ticker'):\n",
    "        g = g.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        roll = g[feature_cols].rolling(window_len, min_periods=window_len)\n",
    "\n",
    "        win_feats = pd.concat(\n",
    "            [\n",
    "                roll.mean().add_suffix('_mean'),\n",
    "                roll.std().add_suffix('_std'),\n",
    "                roll.min().add_suffix('_min'),\n",
    "                roll.max().add_suffix('_max'),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        tmp = pd.concat([g[['ticker', 'date']], win_feats], axis=1).dropna()\n",
    "        frames.append(tmp)\n",
    "\n",
    "    window_df = pd.concat(frames, ignore_index=True)\n",
    "    return window_df\n",
    "\n",
    "window_df = make_window_features(\n",
    "    feat_df,\n",
    "    feature_cols=['log_ret_1d', 'vol_5d', 'vol_20d', 'volume_z', 'range_frac'],\n",
    "    window_len=window_len\n",
    ")\n",
    "\n",
    "print(window_df.shape)\n",
    "window_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f0e59",
   "metadata": {},
   "source": [
    "##### Creating regimes using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  window_df we created above is what we'll use for the Unsuper. learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# feature cols in window_df\n",
    "window_feature_cols = [c for c in window_df.columns if c not in ['ticker', 'date']]\n",
    "\n",
    "X_win = window_df[window_feature_cols].values\n",
    "\n",
    "scaler_win = StandardScaler()\n",
    "X_win_scaled = scaler_win.fit_transform(X_win)\n",
    "\n",
    "k = 6  # first guess for number of regimes; you can try 4, 6, 8, etc.\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "window_df['regime_k6'] = kmeans.fit_predict(X_win_scaled)\n",
    "\n",
    "window_df['regime_k6'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43233fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the regime windows\n",
    "regime_profile = (\n",
    "    window_df\n",
    "    .groupby('regime_k6')[window_feature_cols]\n",
    "    .mean()\n",
    "    .assign(count=window_df.groupby('regime_k6').size())\n",
    ")\n",
    "\n",
    "regime_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2aceb",
   "metadata": {},
   "source": [
    "#### Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358be86b",
   "metadata": {},
   "source": [
    "##### Creating Future Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de652db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating Future Targets\n",
    "\n",
    "# using feat_df so we can line it by ticker and date\n",
    "feat_with_target = feat_df.copy()\n",
    "\n",
    "# number of days ahead\n",
    "horizon = 5\n",
    "\n",
    "def add_targets(g, horizon=horizon):\n",
    "    g = g.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    # cumu sum of 1D log returns\n",
    "    g['cum_log_ret'] = g['log_ret_1d'].cumsum()\n",
    "\n",
    "    # future horizon log return: sum_{t+1..t+horizon} log_ret_1d\n",
    "    # = cum_log_ret at t+horizon minus cum_log_ret at t\n",
    "    g[f'ret_{horizon}d_fwd'] = g['cum_log_ret'].shift(-horizon) - g['cum_log_ret']\n",
    "\n",
    "    return g\n",
    "\n",
    "feat_with_target = (\n",
    "    feat_with_target\n",
    "    .groupby('ticker', group_keys=False)\n",
    "    .apply(add_targets, horizon=horizon)\n",
    ")\n",
    "\n",
    "# binary label: will the horizon return be positive (CLASSIFICXAION)\n",
    "feat_with_target[f'up_{horizon}d'] = (feat_with_target[f'ret_{horizon}d_fwd'] > 0).astype(int)\n",
    "\n",
    "# drop helper column \n",
    "feat_with_target = (\n",
    "    feat_with_target\n",
    "    .drop(columns=['cum_log_ret'])\n",
    "    .dropna(subset=[f'ret_{horizon}d_fwd'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "feat_with_target.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377bf05",
   "metadata": {},
   "source": [
    "##### Merging windows with targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging window features, regimes, and future targets\n",
    "\n",
    "#  window features, regimes, and future targets combined\n",
    "supervised_df = (\n",
    "    window_df\n",
    "    .merge(\n",
    "        feat_with_target[['ticker', 'date', 'ret_5d_fwd', 'up_5d']],\n",
    "        on=['ticker', 'date'],\n",
    "        how='inner'\n",
    "    )\n",
    "    .sort_values(['ticker', 'date'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(supervised_df.shape)\n",
    "supervised_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5cd9a",
   "metadata": {},
   "source": [
    "##### Adding regime transition features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "# getting the previous regime for each window and each ticker\n",
    "supervised_df['regime_prev'] = (supervised_df.groupby('ticker')['regime_k6'].shift(1))\n",
    "\n",
    "# comparing against the previous window to see if there was a regime change\n",
    "supervised_df['regime_changed'] = (\n",
    "\n",
    "    # checking that: \n",
    "    # prev regime not na \n",
    "    # and does not equal current regime\n",
    "    supervised_df['regime_prev'].notna() & (supervised_df['regime_prev'] != supervised_df['regime_k6']).astype(int)\n",
    ")\n",
    "\n",
    "# the first window of the data wont have a regime before it so droping it and reset index\n",
    "supervised_df = supervised_df.dropna(subset=['regime_prev']).reset_index(drop=True)\n",
    "\n",
    "supervised_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab3d69",
   "metadata": {},
   "source": [
    "##### Supervised feature matrix and targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81677eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Still working on this one \n",
    "\n",
    "-Cale\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
